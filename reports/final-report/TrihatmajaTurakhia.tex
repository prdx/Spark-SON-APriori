\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage{listings}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\begin{document}
\thispagestyle{empty}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1.8ex}
\lstset{language=Scala}
\newcommand{\hs}{\hspace{.1in}}

\begin{center}
    \Large{\bf CS 6240}
    \Large{\bf Project Final Report\\}
    \Large{Anak Agung Ngurah Bagus Trihatmaja, Jay Turakhia\\}
    \href{https://github.ccs.neu.edu/prdx/CS6240-Project}{Project Repository}  \\
\end{center}
\medskip

\section{Project Overview}
There are various way to find associations between items in a dataset.
Be it products that people buy together, songs that music aficionados listen together or books that readers would read.
One such interesting association is that of Twitter followership.
It is often seen that similar minded users follow similar users.
In other words, if someone follows a person, who likely it will follow next?
Such relations can be found by solving the \href{https://en.wikipedia.org/wiki/Association_rule_learning}{frequent item set problem} and here we attempt to do the same.
One algorithm to mine the frequent pattern is A-priori.
In this project, we will implement a variation of A-priori, which is \href{http://www.vldb.org/conf/1995/P432.PDF}{SON A-Priori}, to find such relations across millions of followership data quickly and efficiently in parallel.
The algorithm is implemented in Spark.

\section{Input Data}
We are using \href{https://wiki.illinois.edu//wiki/display/forward/Dataset-UDI-TwitterCrawl-Aug2012}{Twitter} data to mine frequent followership patterns.
As described by the data provider.
The dataset contains a single txt file.
In the file, there are 200 millions following relationships among Twitter users.
A following relationship is from a user X to Y , where X follows Y, and Y is followed by X.
The input file represents this relation as follows.\\
$\big[$ Following Relationship 1 $\big]$\\
$\big[$ Following Relationship 2 $\big]$

A following relationship from a user to another user can be represented by the two users' IDs.
Specifically, a following Relationship is represented as follows.

1990012\hspace{1cm}1992012

\section{Pre-processing}
In Frequent item mining, we have transactions and item.
For our project, we define a transaction as an adjacency list of followees and item as a user id.
Therefore, based on our input, we need to combine the followee based on the follower id.
So that we have something as follow: 

$id_1, id_2$ \\
$id_1, id_2, id_3, id_4$ \\
$id_1, id_2, id_3$

Where id is the followee's user Id.
We will not used the follower's user id as it is not relevant for the algorithm.

\subsection{Pseudocode}

\begin{lstlisting}
 val followeeList = input
    .map(x => (x.split("\t")(0), Array(x.split("\t")(1).toInt)))
    .groupByKey()
    .map(y => (y._2.toList.sortBy(y => y)))
\end{lstlisting}

After combining all the edges into adjacency list grouped by the user ID, on the full dataset, we get:

\begin{table}[h!]
    \centering
    \begin{tabular}{||c c||}
        \hline
        Summary & Value \\
        \hline\hline
        Number of rows & 21,603,807 \\
        Maximum length adjacency list & 15,088 \\
        Minimum length of adjacency list & 1 \\
        Average length of adjacency list & 13.2 \\
        \hline
    \end{tabular}
    \caption{Dataset summary}
    \label{table:1}
\end{table}

\section{A-priori}

\subsection {Algorithm Analysis}

In frequent itemset mining, we use one hyperparameter which is minimal support.
Minimal support (min-support) determines how many times a pattern need to exists in the dataset.
The general algorithm for \href{https://www3.cs.stonybrook.edu/~cse634/lecture_notes/07apriori.pdf}{A-priori} is as follows:

\begin{algorithm}[H]
    \caption{A-priori}

    \begin{algorithmic}[1]
        \State $C_K \gets $ Candidate itemset of size $k$
        \State $L_K \gets $ frequent itemset of size $k$
        \State
        \State $L_1 \gets $ List of frequent items $k$
        \State
        \For{$k \gets 1; L_k \neq \emptyset; k++$}
            \State $C_{k+1} \gets $ candidate generate from $L_k$
            \For{each transaction $t$}
                \State increment the count of all candidates in $C_{k+1}$ in $t$
            \EndFor
            \State $L_{k+1} \gets$ candidate in $C_{k+1}$ with minimum support
        \EndFor
    \end{algorithmic}
\end{algorithm}

The join step happens by joining $L_{k-1}$ with itself, producing $C_k$.
While for the pruning, any $k-1$-itemset that is not frequent cannot be a subset of a frequent k-itemset.


\subsection {Pseudo Code}
\begin{lstlisting}
baskets.foreach { basket =>
    // Candidate generation
    _countTable.foreach { item => if (item._2 >= countThreshold) {
            k1FreqItemset += item._1
            freqItemset += item._1
            countTable -= item._1
        }}

    // Increment by one for each transaction
    _countTable.foreach { item =>
        if (item._1.split(",").toSet.subsetOf(basket.toSet)) {
            hasFreqItem = true
            countTable(item._1) += 1
        }}}
\end{lstlisting}

To generate $L_{k+1}$: \\

\begin{lstlisting}
for (i <- 0 to itemSet.size - 1) {
    // Generate combinations
    val c = itemSetList(i).split(",").toSet.subsets(itemPairs - 1).toList

    val outerLoop = new Breaks
    outerLoop.breakable {
        for (j <- 0 to c.size - 1) {
            val freqItemSetList = freqItemset.toList

            innerLoop.breakable {
                for (k <- 0 to freqItemSetList.size - 1) {
                    if (freqItemSetList(k) == c(j).mkString(",")) {
                        isExists = true
                        innerLoop.break()
                    }}}

            if (!isExists) {
                outerLoop.break()
            } else {
                // Generate new L
                countTable += (itemSetList(i) -> 0)
            }
        }
    }
}

countTable
\end{lstlisting}

\section{SON A-Priori Algorithm}

The straight-forward A-priori algorithm does not have the best chances of parallelizing.
A-priori algorithm attempts to iterate through the entire dataset in each pass and accumulates frequent itemsets.
In Map-Reduce, it cause a problem because we have to read the whole data from HDFS every iteration.
In Spark, this problem can be solved by caching the transactions data in memory.
However, if this data is huge, it might not fit in the memory.

The SON A-Priori implementation in the other hand uses the idea of local computation.
SON A-Priori partitions the data and attempts to look at smaller subsets of the input and the local winners are sent across to a global reducer (or equivalent) so that global frequent itemsets can be computed.
It relies on a principle that "an itemset cannot be a frequent itemset globally if it is not so in at least one partition".
It is easy to organize the input as small chunks and deploy worker machines in parallel to mine the local frequent itemsets.
After this phase, all the frequent itemsets can be accumulated and another pass can be run to find the global frequent itemsets.

We implement the first task to generate local frequent itemsets (phase 1) by running A-Priori on the partition data.
For each partition, we generate the frequent itemsets entirely in-memory.
From MapReduce perspective, we can say that this task is map only task.
The input of this task is the list of followee.

The second major task is to find the global frequent winner itemset from local itemsets (phase 2).
We are removing the candidates from the first phase that are false-positives.
The candidate itemsets computed in the first phase will contain all the frequent itemsets but may also contain itemsets that are not frequent when we consider the entire dataset.

\subsection{Pseudocode}

For the phase 2, the pseudocode is as follow.

\begin{lstlisting}
def execute(_baskets, sp, myLocals) = {

    val baskets: List[List[String]] = _baskets.map(
                basket => basket.map(x => x.toString)
            )
    val len = baskets.length
    val countThreshold = math.ceil(len * sp)
    val globalList  = ListBuffer[(String, Int)]()

    var bsk = Set[String]()

    baskets.foreach { basket =>
        bsk = basket.toSet
        myLocals.foreach { local =>
            var lsk = local._1.split(",").toSet
            if (lsk.subsetOf(bsk))
                globalList += ((local._1,1))
        }
    }

    return globalList.iterator
}
\end{lstlisting}

\subsection{Experiments}

We run A-Priori on the partition of the data.
The threshold is a hard limit set to ignore inputs that we do not want to be included in the final count.
Such pruning, as presented in Step 11 of the algorithm, reduces the data to be transferred from individual map task to the global accumulator


The second one, is to change the number of minimal support.
In this report, we do it for few different values.
Smaller support will give us larger amount of data during the phase 1, thus more computation.
However, it also gives us more patterns of the data as in real world--less likely a pattern exists for min-support of $.9$.

\subsubsection{Speedup}


\begin{table}[h!]
    \centering
    \begin{tabular}{||c c||}
        \hline
        min-support & Running time \\
        \hline\hline
        $9 \times 10^{-4}$ & 14 minutes \\
        $75 \times 10^{-6}$ & $>$ 5 hours (terminated) \\
        \hline
    \end{tabular}
    \caption{Running time for small cluster with various min-support}
    \label{table:2}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabular}{||c c||}
        \hline
        min-support & Running time \\
        \hline\hline
        $9 \times 10^{-4}$ & 8 minutes \\
        $75 \times 10^{-6}$ & $>$ 5 hours (terminated) \\
        \hline
    \end{tabular}
    \caption{Running time for large cluster with various min-support}
    \label{table:3}
\end{table}

\subsubsection{Scalability}

As we measure the scalability, the lower the min-support the longer it takes to run.
We expect a longer duration to finish for larger input.
The way we reduce our input by using the max id method as we previously used in the assignments.
That is, we only include an edge if both vertices id are below our max id.
For the scalability, we fix the min-support of $.75$.

\begin{table}[h!]
    \centering
    \begin{tabular}{||c c c||}
        \hline
        Number of clusters & Max id & Running time \\
        \hline\hline
        5 & 10,000 & 1.4 minutes \\
        5 & 100,000 & 1.4 minutes \\
        \hline
        10 & 10,000 & 1.4 minutes \\
        10 & 100,000 & 1.4 minutes \\
        \hline
    \end{tabular}
    \caption{Running time for fixed threshold with various number of clusters and max id}
    \label{table:4}
\end{table}

We see that the running time is almost the same. 
That is because of the min-support is too high, causing no candidates is generated and therefore the job ends sooner.

\subsubsection{Result Sample}

\begin{table}[h!]
    \centering
    \begin{tabular}{||c c||}
        \hline
        min-support & Number of patterns produced \\
        \hline\hline
        $.75$ & 0 \\
        $9 \times 10^{-4}$ & 6 \\
        \hline
    \end{tabular}
    \caption{Number of patterns produced with different min-support}
    \label{table:5}
\end{table}

\subsubsection{The effect of a skewed data}

In our exploratory data analysis during the pre-processing, we have seen that there is a very long adjacency list compared to the average.
This indicates our data can be skewed.
In SON-APriori, this can be a problem--imagine in one partition there is a skewed data, then that cluster will take longer time to finish.
If there those long adjancency lists are not frequent globally, we basically waste the computation.

To illustrate, we compare the running time of the full dataset with the dataset where we truncate the adjacency list to only maximum 10 edges.
We then fix a high min-support just to see the result.
With a full dataset our job runs for 4.7 minutes while after truncated we receive 3.7 minutes. 

To avoid such cases, we repartition our data to make each partition smaller after the pre-processing is done.
Fine-grained partition allows smaller memory usage with better load distribution.
Ideally, smaller partition means faster local computation as well.
However, we sacrifice the network traffic with this approach as we need to re-shuffle our data again.

\end{document}
