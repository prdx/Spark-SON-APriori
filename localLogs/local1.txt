mvn clean package
[[1;34mINFO[m] Scanning for projects...
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] [1mBuilding project 1.0[m
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-clean-plugin:2.5:clean[m [1m(default-clean)[m @ [36mproject[0;1m ---[m
[[1;34mINFO[m] Deleting /home/jay/CS6240-Project/target
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mproject[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/jay/CS6240-Project/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.1:compile[m [1m(default-compile)[m @ [36mproject[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mscala-maven-plugin:3.3.1:compile[m [1m(default)[m @ [36mproject[0;1m ---[m
[[1;33mWARNING[m]  Expected all dependencies to require Scala version: 2.11.12
[[1;33mWARNING[m]  cs6240:project:1.0 requires scala version: 2.11.12
[[1;33mWARNING[m]  com.twitter:chill_2.11:0.8.4 requires scala version: 2.11.8
[[1;33mWARNING[m] Multiple versions of scala libraries detected!
[[1;34mINFO[m] /home/jay/CS6240-Project/src/main/scala:-1: info: compiling
[[1;34mINFO[m] Compiling 3 source files to /home/jay/CS6240-Project/target/classes at 1553831160810
[[1;34mINFO[m] prepare-compile in 0 s
[[1;34mINFO[m] compile in 7 s
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mproject[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/jay/CS6240-Project/src/test/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.1:testCompile[m [1m(default-testCompile)[m @ [36mproject[0;1m ---[m
[[1;34mINFO[m] No sources to compile
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:2.12.4:test[m [1m(default-test)[m @ [36mproject[0;1m ---[m
[[1;34mINFO[m] No tests to run.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:2.4:jar[m [1m(default-jar)[m @ [36mproject[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/jay/CS6240-Project/target/project-1.0.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-shade-plugin:3.1.0:shade[m [1m(default)[m @ [36mproject[0;1m ---[m
[[1;34mINFO[m] Replacing original artifact with shaded artifact.
[[1;34mINFO[m] Replacing /home/jay/CS6240-Project/target/project-1.0.jar with /home/jay/CS6240-Project/target/project-1.0-shaded.jar
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] [1;32mBUILD SUCCESS[m
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] Total time: 13.151 s
[[1;34mINFO[m] Finished at: 2019-03-28T23:46:08-04:00
[[1;34mINFO[m] Final Memory: 31M/453M
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
cp target/project-1.0.jar project.jar
rm -rf output
spark-submit --class project.ProjectImpl --master local[4] --name "Project" project.jar input output 0
2019-03-28 23:46:10 WARN  Utils:66 - Your hostname, jay-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)
2019-03-28 23:46:10 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address
2019-03-28 23:46:10 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-03-28 23:46:11 INFO  SparkContext:54 - Running Spark version 2.4.0
2019-03-28 23:46:11 INFO  SparkContext:54 - Submitted application: Project - APriori
2019-03-28 23:46:11 INFO  SecurityManager:54 - Changing view acls to: jay
2019-03-28 23:46:11 INFO  SecurityManager:54 - Changing modify acls to: jay
2019-03-28 23:46:11 INFO  SecurityManager:54 - Changing view acls groups to: 
2019-03-28 23:46:11 INFO  SecurityManager:54 - Changing modify acls groups to: 
2019-03-28 23:46:11 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jay); groups with view permissions: Set(); users  with modify permissions: Set(jay); groups with modify permissions: Set()
2019-03-28 23:46:11 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 43753.
2019-03-28 23:46:11 INFO  SparkEnv:54 - Registering MapOutputTracker
2019-03-28 23:46:11 INFO  SparkEnv:54 - Registering BlockManagerMaster
2019-03-28 23:46:11 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-03-28 23:46:11 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up
2019-03-28 23:46:11 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-8fa3a818-738d-46aa-8edb-ad6d996174f2
2019-03-28 23:46:11 INFO  MemoryStore:54 - MemoryStore started with capacity 366.3 MB
2019-03-28 23:46:11 INFO  SparkEnv:54 - Registering OutputCommitCoordinator
2019-03-28 23:46:11 INFO  log:192 - Logging initialized @2535ms
2019-03-28 23:46:11 INFO  Server:351 - jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-03-28 23:46:11 INFO  Server:419 - Started @2636ms
2019-03-28 23:46:11 INFO  AbstractConnector:278 - Started ServerConnector@6ab72419{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-03-28 23:46:11 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.
2019-03-28 23:46:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@cc62a3b{/jobs,null,AVAILABLE,@Spark}
2019-03-28 23:46:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@39c11e6c{/jobs/json,null,AVAILABLE,@Spark}
2019-03-28 23:46:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@324dcd31{/jobs/job,null,AVAILABLE,@Spark}
2019-03-28 23:46:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@72bca894{/jobs/job/json,null,AVAILABLE,@Spark}
2019-03-28 23:46:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@433ffad1{/stages,null,AVAILABLE,@Spark}
2019-03-28 23:46:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1fc793c2{/stages/json,null,AVAILABLE,@Spark}
2019-03-28 23:46:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2575f671{/stages/stage,null,AVAILABLE,@Spark}
2019-03-28 23:46:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2d35442b{/stages/stage/json,null,AVAILABLE,@Spark}
2019-03-28 23:46:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@27f9e982{/stages/pool,null,AVAILABLE,@Spark}
2019-03-28 23:46:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4593ff34{/stages/pool/json,null,AVAILABLE,@Spark}
2019-03-28 23:46:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@37d3d232{/storage,null,AVAILABLE,@Spark}
2019-03-28 23:46:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@30c0ccff{/storage/json,null,AVAILABLE,@Spark}
2019-03-28 23:46:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@581d969c{/storage/rdd,null,AVAILABLE,@Spark}
2019-03-28 23:46:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@22db8f4{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-03-28 23:46:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2b46a8c1{/environment,null,AVAILABLE,@Spark}
2019-03-28 23:46:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1d572e62{/environment/json,null,AVAILABLE,@Spark}
2019-03-28 23:46:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@29caf222{/executors,null,AVAILABLE,@Spark}
2019-03-28 23:46:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@46cf05f7{/executors/json,null,AVAILABLE,@Spark}
2019-03-28 23:46:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5851bd4f{/executors/threadDump,null,AVAILABLE,@Spark}
2019-03-28 23:46:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7cd1ac19{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-03-28 23:46:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2f40a43{/static,null,AVAILABLE,@Spark}
2019-03-28 23:46:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5949eba8{/,null,AVAILABLE,@Spark}
2019-03-28 23:46:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6e0ff644{/api,null,AVAILABLE,@Spark}
2019-03-28 23:46:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2d0566ba{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-03-28 23:46:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@733037{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-03-28 23:46:12 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://10.0.2.15:4040
2019-03-28 23:46:12 INFO  SparkContext:54 - Added JAR file:/home/jay/CS6240-Project/project.jar at spark://10.0.2.15:43753/jars/project.jar with timestamp 1553831172083
2019-03-28 23:46:12 INFO  Executor:54 - Starting executor ID driver on host localhost
2019-03-28 23:46:12 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45399.
2019-03-28 23:46:12 INFO  NettyBlockTransferService:54 - Server created on 10.0.2.15:45399
2019-03-28 23:46:12 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-03-28 23:46:12 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 10.0.2.15, 45399, None)
2019-03-28 23:46:12 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.0.2.15:45399 with 366.3 MB RAM, BlockManagerId(driver, 10.0.2.15, 45399, None)
2019-03-28 23:46:12 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 10.0.2.15, 45399, None)
2019-03-28 23:46:12 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 45399, None)
2019-03-28 23:46:12 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@13cd7ea5{/metrics/json,null,AVAILABLE,@Spark}
2019-03-28 23:46:13 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 394.1 KB, free 365.9 MB)
2019-03-28 23:46:13 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.8 KB, free 365.9 MB)
2019-03-28 23:46:13 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on 10.0.2.15:45399 (size: 35.8 KB, free: 366.3 MB)
2019-03-28 23:46:13 INFO  SparkContext:54 - Created broadcast 0 from textFile at ProjectImpl.scala:19
2019-03-28 23:46:13 INFO  FileInputFormat:256 - Total input files to process : 1
2019-03-28 23:46:13 INFO  deprecation:1297 - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
2019-03-28 23:46:13 INFO  HadoopMapRedCommitProtocol:54 - Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2019-03-28 23:46:13 INFO  FileOutputCommitter:123 - File Output Committer Algorithm version is 1
2019-03-28 23:46:13 INFO  FileOutputCommitter:138 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2019-03-28 23:46:13 INFO  SparkContext:54 - Starting job: runJob at SparkHadoopWriter.scala:78
2019-03-28 23:46:13 INFO  DAGScheduler:54 - Registering RDD 2 (map at ProjectImpl.scala:27)
2019-03-28 23:46:13 INFO  DAGScheduler:54 - Got job 0 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions
2019-03-28 23:46:13 INFO  DAGScheduler:54 - Final stage: ResultStage 1 (runJob at SparkHadoopWriter.scala:78)
2019-03-28 23:46:13 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 0)
2019-03-28 23:46:13 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 0)
2019-03-28 23:46:13 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at map at ProjectImpl.scala:27), which has no missing parents
2019-03-28 23:46:13 INFO  MemoryStore:54 - Block broadcast_1 stored as values in memory (estimated size 5.6 KB, free 365.9 MB)
2019-03-28 23:46:13 INFO  MemoryStore:54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.1 KB, free 365.9 MB)
2019-03-28 23:46:13 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on 10.0.2.15:45399 (size: 3.1 KB, free: 366.3 MB)
2019-03-28 23:46:13 INFO  SparkContext:54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-03-28 23:46:14 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at map at ProjectImpl.scala:27) (first 15 tasks are for partitions Vector(0))
2019-03-28 23:46:14 INFO  TaskSchedulerImpl:54 - Adding task set 0.0 with 1 tasks
2019-03-28 23:46:14 INFO  TaskSetManager:54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes)
2019-03-28 23:46:14 INFO  Executor:54 - Running task 0.0 in stage 0.0 (TID 0)
2019-03-28 23:46:14 INFO  Executor:54 - Fetching spark://10.0.2.15:43753/jars/project.jar with timestamp 1553831172083
2019-03-28 23:46:14 INFO  TransportClientFactory:267 - Successfully created connection to /10.0.2.15:43753 after 49 ms (0 ms spent in bootstraps)
2019-03-28 23:46:14 INFO  Utils:54 - Fetching spark://10.0.2.15:43753/jars/project.jar to /tmp/spark-1fe12423-b6b6-4a61-8a3c-fa0850651924/userFiles-b5bf1a1d-8540-4145-a84b-dd4d279b5d5e/fetchFileTemp8044184103707761263.tmp
2019-03-28 23:46:14 INFO  Executor:54 - Adding file:/tmp/spark-1fe12423-b6b6-4a61-8a3c-fa0850651924/userFiles-b5bf1a1d-8540-4145-a84b-dd4d279b5d5e/project.jar to class loader
2019-03-28 23:46:14 INFO  HadoopRDD:54 - Input split: file:/home/jay/CS6240-Project/input/smallestNetwork.txt:0+131863
2019-03-28 23:46:14 INFO  Executor:54 - Finished task 0.0 in stage 0.0 (TID 0). 1064 bytes result sent to driver
2019-03-28 23:46:14 INFO  TaskSetManager:54 - Finished task 0.0 in stage 0.0 (TID 0) in 893 ms on localhost (executor driver) (1/1)
2019-03-28 23:46:14 INFO  TaskSchedulerImpl:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-03-28 23:46:14 INFO  DAGScheduler:54 - ShuffleMapStage 0 (map at ProjectImpl.scala:27) finished in 1.029 s
2019-03-28 23:46:14 INFO  DAGScheduler:54 - looking for newly runnable stages
2019-03-28 23:46:14 INFO  DAGScheduler:54 - running: Set()
2019-03-28 23:46:15 INFO  DAGScheduler:54 - waiting: Set(ResultStage 1)
2019-03-28 23:46:15 INFO  DAGScheduler:54 - failed: Set()
2019-03-28 23:46:15 INFO  DAGScheduler:54 - Submitting ResultStage 1 (MapPartitionsRDD[6] at saveAsTextFile at ProjectImpl.scala:29), which has no missing parents
2019-03-28 23:46:15 INFO  MemoryStore:54 - Block broadcast_2 stored as values in memory (estimated size 110.1 KB, free 365.8 MB)
2019-03-28 23:46:15 INFO  MemoryStore:54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 40.6 KB, free 365.7 MB)
2019-03-28 23:46:15 INFO  BlockManagerInfo:54 - Added broadcast_2_piece0 in memory on 10.0.2.15:45399 (size: 40.6 KB, free: 366.2 MB)
2019-03-28 23:46:15 INFO  SparkContext:54 - Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-03-28 23:46:15 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at saveAsTextFile at ProjectImpl.scala:29) (first 15 tasks are for partitions Vector(0))
2019-03-28 23:46:15 INFO  TaskSchedulerImpl:54 - Adding task set 1.0 with 1 tasks
2019-03-28 23:46:15 INFO  TaskSetManager:54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7662 bytes)
2019-03-28 23:46:15 INFO  Executor:54 - Running task 0.0 in stage 1.0 (TID 1)
2019-03-28 23:46:15 INFO  ShuffleBlockFetcherIterator:54 - Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-03-28 23:46:15 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 10 ms
2019-03-28 23:46:15 INFO  BlockManagerInfo:54 - Removed broadcast_1_piece0 on 10.0.2.15:45399 in memory (size: 3.1 KB, free: 366.2 MB)
2019-03-28 23:54:52 ERROR Executor:91 - Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:68)
	at java.lang.StringBuilder.<init>(StringBuilder.java:101)
	at scala.collection.mutable.StringBuilder.<init>(StringBuilder.scala:49)
	at scala.collection.mutable.StringBuilder.<init>(StringBuilder.scala:54)
	at scala.collection.TraversableOnce$class.mkString(TraversableOnce.scala:323)
	at scala.collection.AbstractTraversable.mkString(Traversable.scala:104)
	at scala.collection.TraversableOnce$class.mkString(TraversableOnce.scala:325)
	at scala.collection.AbstractTraversable.mkString(Traversable.scala:104)
	at project.algorithm.Apriori$$anonfun$getCountTable$3.apply(Apriori.scala:94)
	at project.algorithm.Apriori$$anonfun$getCountTable$3.apply(Apriori.scala:93)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at project.algorithm.Apriori$.getCountTable(Apriori.scala:93)
	at project.algorithm.Apriori$.execute(Apriori.scala:34)
	at project.ProjectImpl$$anonfun$main$1.apply(ProjectImpl.scala:29)
	at project.ProjectImpl$$anonfun$main$1.apply(ProjectImpl.scala:29)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-03-28 23:54:52 ERROR SparkUncaughtExceptionHandler:91 - Uncaught exception in thread Thread[Executor task launch worker for task 1,5,main]
java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:68)
	at java.lang.StringBuilder.<init>(StringBuilder.java:101)
	at scala.collection.mutable.StringBuilder.<init>(StringBuilder.scala:49)
	at scala.collection.mutable.StringBuilder.<init>(StringBuilder.scala:54)
	at scala.collection.TraversableOnce$class.mkString(TraversableOnce.scala:323)
	at scala.collection.AbstractTraversable.mkString(Traversable.scala:104)
	at scala.collection.TraversableOnce$class.mkString(TraversableOnce.scala:325)
	at scala.collection.AbstractTraversable.mkString(Traversable.scala:104)
	at project.algorithm.Apriori$$anonfun$getCountTable$3.apply(Apriori.scala:94)
	at project.algorithm.Apriori$$anonfun$getCountTable$3.apply(Apriori.scala:93)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at project.algorithm.Apriori$.getCountTable(Apriori.scala:93)
	at project.algorithm.Apriori$.execute(Apriori.scala:34)
	at project.ProjectImpl$$anonfun$main$1.apply(ProjectImpl.scala:29)
	at project.ProjectImpl$$anonfun$main$1.apply(ProjectImpl.scala:29)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-03-28 23:54:52 INFO  SparkContext:54 - Invoking stop() from shutdown hook
2019-03-28 23:54:52 WARN  TaskSetManager:66 - Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:68)
	at java.lang.StringBuilder.<init>(StringBuilder.java:101)
	at scala.collection.mutable.StringBuilder.<init>(StringBuilder.scala:49)
	at scala.collection.mutable.StringBuilder.<init>(StringBuilder.scala:54)
	at scala.collection.TraversableOnce$class.mkString(TraversableOnce.scala:323)
	at scala.collection.AbstractTraversable.mkString(Traversable.scala:104)
	at scala.collection.TraversableOnce$class.mkString(TraversableOnce.scala:325)
	at scala.collection.AbstractTraversable.mkString(Traversable.scala:104)
	at project.algorithm.Apriori$$anonfun$getCountTable$3.apply(Apriori.scala:94)
	at project.algorithm.Apriori$$anonfun$getCountTable$3.apply(Apriori.scala:93)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at project.algorithm.Apriori$.getCountTable(Apriori.scala:93)
	at project.algorithm.Apriori$.execute(Apriori.scala:34)
	at project.ProjectImpl$$anonfun$main$1.apply(ProjectImpl.scala:29)
	at project.ProjectImpl$$anonfun$main$1.apply(ProjectImpl.scala:29)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-03-28 23:54:52 ERROR TaskSetManager:70 - Task 0 in stage 1.0 failed 1 times; aborting job
2019-03-28 23:54:52 INFO  TaskSchedulerImpl:54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-03-28 23:54:52 INFO  TaskSchedulerImpl:54 - Cancelling stage 1
2019-03-28 23:54:52 INFO  TaskSchedulerImpl:54 - Killing all running tasks in stage 1: Stage cancelled
2019-03-28 23:54:52 INFO  DAGScheduler:54 - ResultStage 1 (runJob at SparkHadoopWriter.scala:78) failed in 517.549 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:68)
	at java.lang.StringBuilder.<init>(StringBuilder.java:101)
	at scala.collection.mutable.StringBuilder.<init>(StringBuilder.scala:49)
	at scala.collection.mutable.StringBuilder.<init>(StringBuilder.scala:54)
	at scala.collection.TraversableOnce$class.mkString(TraversableOnce.scala:323)
	at scala.collection.AbstractTraversable.mkString(Traversable.scala:104)
	at scala.collection.TraversableOnce$class.mkString(TraversableOnce.scala:325)
	at scala.collection.AbstractTraversable.mkString(Traversable.scala:104)
	at project.algorithm.Apriori$$anonfun$getCountTable$3.apply(Apriori.scala:94)
	at project.algorithm.Apriori$$anonfun$getCountTable$3.apply(Apriori.scala:93)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at project.algorithm.Apriori$.getCountTable(Apriori.scala:93)
	at project.algorithm.Apriori$.execute(Apriori.scala:34)
	at project.ProjectImpl$$anonfun$main$1.apply(ProjectImpl.scala:29)
	at project.ProjectImpl$$anonfun$main$1.apply(ProjectImpl.scala:29)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
2019-03-28 23:54:52 INFO  AbstractConnector:318 - Stopped Spark@6ab72419{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-03-28 23:54:52 INFO  SparkUI:54 - Stopped Spark web UI at http://10.0.2.15:4040
2019-03-28 23:54:52 INFO  DAGScheduler:54 - Job 0 failed: runJob at SparkHadoopWriter.scala:78, took 518.708782 s
2019-03-28 23:54:52 ERROR SparkHadoopWriter:91 - Aborting job job_20190328234613_0006.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:68)
	at java.lang.StringBuilder.<init>(StringBuilder.java:101)
	at scala.collection.mutable.StringBuilder.<init>(StringBuilder.scala:49)
	at scala.collection.mutable.StringBuilder.<init>(StringBuilder.scala:54)
	at scala.collection.TraversableOnce$class.mkString(TraversableOnce.scala:323)
	at scala.collection.AbstractTraversable.mkString(Traversable.scala:104)
	at scala.collection.TraversableOnce$class.mkString(TraversableOnce.scala:325)
	at scala.collection.AbstractTraversable.mkString(Traversable.scala:104)
	at project.algorithm.Apriori$$anonfun$getCountTable$3.apply(Apriori.scala:94)
	at project.algorithm.Apriori$$anonfun$getCountTable$3.apply(Apriori.scala:93)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at project.algorithm.Apriori$.getCountTable(Apriori.scala:93)
	at project.algorithm.Apriori$.execute(Apriori.scala:34)
	at project.ProjectImpl$$anonfun$main$1.apply(ProjectImpl.scala:29)
	at project.ProjectImpl$$anonfun$main$1.apply(ProjectImpl.scala:29)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)
	at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)
	at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1499)
	at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)
	at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1478)
	at project.ProjectImpl$.main(ProjectImpl.scala:29)
	at project.ProjectImpl.main(ProjectImpl.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:849)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:68)
	at java.lang.StringBuilder.<init>(StringBuilder.java:101)
	at scala.collection.mutable.StringBuilder.<init>(StringBuilder.scala:49)
	at scala.collection.mutable.StringBuilder.<init>(StringBuilder.scala:54)
	at scala.collection.TraversableOnce$class.mkString(TraversableOnce.scala:323)
	at scala.collection.AbstractTraversable.mkString(Traversable.scala:104)
	at scala.collection.TraversableOnce$class.mkString(TraversableOnce.scala:325)
	at scala.collection.AbstractTraversable.mkString(Traversable.scala:104)
	at project.algorithm.Apriori$$anonfun$getCountTable$3.apply(Apriori.scala:94)
	at project.algorithm.Apriori$$anonfun$getCountTable$3.apply(Apriori.scala:93)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at project.algorithm.Apriori$.getCountTable(Apriori.scala:93)
	at project.algorithm.Apriori$.execute(Apriori.scala:34)
	at project.ProjectImpl$$anonfun$main$1.apply(ProjectImpl.scala:29)
	at project.ProjectImpl$$anonfun$main$1.apply(ProjectImpl.scala:29)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-03-28 23:54:52 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!
2019-03-28 23:54:52 INFO  MemoryStore:54 - MemoryStore cleared
2019-03-28 23:54:52 INFO  BlockManager:54 - BlockManager stopped
2019-03-28 23:54:52 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped
2019-03-28 23:54:52 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!
2019-03-28 23:54:52 INFO  SparkContext:54 - Successfully stopped SparkContext
2019-03-28 23:54:52 INFO  ShutdownHookManager:54 - Shutdown hook called
2019-03-28 23:54:52 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-3c2d1cbe-05a8-44cd-9499-fd9b4aa601ca
2019-03-28 23:54:52 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-1fe12423-b6b6-4a61-8a3c-fa0850651924
Makefile:42: recipe for target 'local' failed
